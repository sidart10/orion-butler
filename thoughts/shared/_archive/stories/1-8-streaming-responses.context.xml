<?xml version="1.0" encoding="UTF-8"?>
<!--
  STORY CONTEXT: 1-8-streaming-responses
  Generated: 2026-01-15
  Purpose: Comprehensive context for DEV agent implementation

  This file provides all information needed to implement Story 1.8 without
  requiring additional lookups of planning artifacts.
-->
<story-context story-id="1-8-streaming-responses" epic="1" status="ready-for-dev">

  <!-- ============================================================ -->
  <!-- SECTION 1: STORY OVERVIEW -->
  <!-- ============================================================ -->
  <story>
    <title>Streaming Responses</title>
    <epic-name>Foundation and First Chat</epic-name>
    <status>ready-for-dev</status>

    <user-story>
      As a user,
      I want to see Claude's response appear word-by-word,
      So that I get immediate feedback and the experience feels responsive.
    </user-story>

    <business-value>
      Streaming provides immediate visual feedback, making the AI feel responsive
      and engaging. This is critical for user satisfaction and aligns with modern
      LLM chat expectations.
    </business-value>

    <dependencies>
      <dependency story="1-5" name="Agent Server Process" required="true">
        Provides SSE endpoint stub, Express routing, Tauri child process management
      </dependency>
      <dependency story="1-6" name="Chat Message Storage" required="true">
        Provides message persistence via save_assistant_message Tauri command
      </dependency>
      <dependency story="1-7" name="Claude Integration" required="true">
        Provides ClaudeClient class, error handling patterns, chat store foundation
      </dependency>
    </dependencies>
  </story>

  <!-- ============================================================ -->
  <!-- SECTION 2: ACCEPTANCE CRITERIA -->
  <!-- ============================================================ -->
  <acceptance-criteria>

    <ac id="AC1" title="SSE Connection Established">
      <given>I send a message to Claude</given>
      <when>Claude starts responding</when>
      <then>
        - Tokens stream in real-time via SSE
        - Agent Server establishes SSE connection to frontend
        - Each token is delivered as a separate SSE event
      </then>
      <architecture-refs>
        - Streaming Protocol: SSE (Server-Sent Events)
        - Agent Server Port: localhost:3001
      </architecture-refs>
    </ac>

    <ac id="AC2" title="Typing Indicator">
      <given>I send a message to Claude</given>
      <when>Claude starts responding</when>
      <then>
        - I see a typing indicator before the first token arrives
        - The indicator matches the Orion Design System styling
      </then>
      <design-tokens>
        - orion-primary: #D4AF37 (gold bounce dots)
        - orion-fg/50: 50% opacity text
        - tracking-editorial: 0.25em letter spacing
      </design-tokens>
    </ac>

    <ac id="AC3" title="First Token Latency">
      <given>I send a message to Claude</given>
      <when>Claude starts responding</when>
      <then>
        - Latency to first token is less than 500ms p95
        - Timing is measurable via performance API
      </then>
      <nfr-ref>NFR-P001: First token latency less than 500ms p95</nfr-ref>
    </ac>

    <ac id="AC4" title="Smooth Token Rendering">
      <given>Streaming is in progress</given>
      <when>Tokens arrive</when>
      <then>
        - Tokens append smoothly without flickering
        - No visual artifacts or jumps occur during rendering
      </then>
    </ac>

    <ac id="AC5" title="Auto-Scroll During Streaming">
      <given>Streaming is in progress</given>
      <when>Tokens arrive</when>
      <then>
        - Chat auto-scrolls to show new content
        - Scroll position stays at bottom during streaming
        - If user scrolls up manually, auto-scroll pauses
      </then>
    </ac>

    <ac id="AC6" title="Stream Completion">
      <given>Streaming completes</given>
      <when>Final token arrives</when>
      <then>
        - Typing indicator disappears
        - Message is marked as complete in database
        - Token counts (input/output) are persisted
      </then>
    </ac>

    <ac id="AC7" title="Long Response Handling">
      <given>Claude generates a long response (1000+ tokens)</given>
      <when>Streaming</when>
      <then>
        - Tokens render smoothly throughout
        - Memory usage remains stable
        - UI remains responsive
      </then>
    </ac>

    <ac id="AC8" title="Tauri IPC Event Forwarding">
      <given>Agent Server receives SSE events</given>
      <when>Events are received</when>
      <then>
        - Events are forwarded to WebView via Tauri events
        - Frontend receives events via listen() API
      </then>
    </ac>

  </acceptance-criteria>

  <!-- ============================================================ -->
  <!-- SECTION 3: ARCHITECTURE CONTEXT -->
  <!-- ============================================================ -->
  <architecture>

    <streaming-protocol>
      <type>Server-Sent Events (SSE)</type>
      <rationale>
        SSE is simpler than WebSocket, unidirectional (server to client),
        perfect for streaming text. Native browser support via fetch + ReadableStream.
      </rationale>

      <event-format>
        <![CDATA[
event: <event-type>
data: {"key": "value", ...}

        ]]>
      </event-format>

      <event-types>
        <event type="connected" description="Stream initialized with streamId and timestamp"/>
        <event type="text" description="Text token content"/>
        <event type="thinking" description="Extended thinking content (future)"/>
        <event type="tool_start" description="Tool execution started"/>
        <event type="tool_input" description="Tool receiving input"/>
        <event type="tool_complete" description="Tool execution finished"/>
        <event type="complete" description="Stream finished with token counts"/>
        <event type="error" description="Error occurred with message and code"/>
      </event-types>
    </streaming-protocol>

    <data-flow>
      <![CDATA[
User Input
    |
    v
Frontend (React) --sendMessageStreaming()--> Chat Store
    |
    v
Chat Store --invoke('start_agent_stream')--> Tauri Rust Command
    |
    v
Tauri Rust --HTTP POST--> Agent Server (:3001/api/stream/start)
    |
    v
Agent Server --client.messages.stream()--> Claude API
    |
    v
Claude API ==SSE==> Agent Server
    |
    v
Agent Server ==SSE Response==> Tauri Rust
    |
    v
Tauri Rust --app.emit('agent:stream:xxx')--> Frontend
    |
    v
Frontend (listen) --> Chat Store --> UI Update
      ]]>
    </data-flow>

    <type-definitions>
      <![CDATA[
// src/types/streaming.ts

export type StreamEventType =
  | 'connected'
  | 'thinking'
  | 'text'
  | 'tool_start'
  | 'tool_input'
  | 'tool_complete'
  | 'tool_error'
  | 'complete'
  | 'error';

export interface StreamEvent {
  type: StreamEventType;
  timestamp?: number;

  // Connected event
  streamId?: string;

  // Text content
  content?: string;

  // Tool events
  toolId?: string;
  toolName?: string;
  toolInput?: Record<string, unknown>;
  toolResult?: unknown;
  toolError?: string;

  // Complete event
  inputTokens?: number;
  outputTokens?: number;

  // Error
  message?: string;
  code?: string;
}

export interface StreamState {
  isStreaming: boolean;
  currentStreamId: string | null;
  firstTokenReceived: boolean;
  firstTokenLatencyMs: number | null;
  totalTokens: number;
  startTime: number | null;
}
      ]]>
    </type-definitions>

    <sse-headers>
      <header name="Content-Type">text/event-stream</header>
      <header name="Cache-Control">no-cache</header>
      <header name="Connection">keep-alive</header>
      <header name="X-Accel-Buffering">no</header>
    </sse-headers>

    <agent-server>
      <port>3001</port>
      <protocol>HTTP/SSE</protocol>
      <endpoint-stream>/api/stream/start (POST)</endpoint-stream>
      <endpoint-stream-get>/api/stream/:streamId (GET for smaller payloads)</endpoint-stream-get>
    </agent-server>

  </architecture>

  <!-- ============================================================ -->
  <!-- SECTION 4: TASK BREAKDOWN -->
  <!-- ============================================================ -->
  <tasks>

    <task id="1" name="Extend Claude Client for Streaming" acs="AC1,AC3">
      <description>Add streaming method to ClaudeClient class</description>
      <files>
        <file action="modify">agent-server/src/services/claude-client.ts</file>
      </files>
      <subtasks>
        <subtask>1.1 Add sendMessageStreaming method</subtask>
        <subtask>1.2 Implement stream event handling for text deltas</subtask>
        <subtask>1.3 Handle thinking deltas for extended thinking support</subtask>
        <subtask>1.4 Define StreamCallbacks interface</subtask>
      </subtasks>
      <code-pattern>
        <![CDATA[
import Anthropic from '@anthropic-ai/sdk';
import type { MessageStreamEvent } from '@anthropic-ai/sdk/lib/MessageStream';

export interface StreamCallbacks {
  onText: (text: string) => void;
  onThinking?: (thinking: string) => void;
  onToolStart?: (toolId: string, toolName: string) => void;
  onToolInput?: (toolId: string, input: Record<string, unknown>) => void;
  onToolComplete?: (toolId: string, result: unknown) => void;
  onComplete: (usage: { inputTokens: number; outputTokens: number }) => void;
  onError: (error: Error) => void;
}

// Add to ClaudeClient class:
async sendMessageStreaming(
  userMessage: string,
  conversationHistory: ConversationMessage[] = [],
  systemPrompt: string | undefined,
  callbacks: StreamCallbacks,
): Promise<void> {
  // Implementation uses client.messages.stream() with for await
}
        ]]>
      </code-pattern>
    </task>

    <task id="2" name="Create SSE Streaming Endpoint" acs="AC1">
      <description>Replace placeholder SSE endpoint with real implementation</description>
      <files>
        <file action="modify">agent-server/src/routes/stream.ts</file>
      </files>
      <subtasks>
        <subtask>2.1 Replace placeholder SSE endpoint from Story 1.5</subtask>
        <subtask>2.2 Implement proper SSE event formatting with sendSSE helper</subtask>
        <subtask>2.3 Add error handling with appropriate error codes</subtask>
        <subtask>2.4 Support both GET and POST endpoints</subtask>
      </subtasks>
      <error-codes>
        <code name="RATE_LIMITED" http="429"/>
        <code name="AUTH_ERROR" http="401"/>
        <code name="PERMISSION_ERROR" http="403"/>
        <code name="API_ERROR" http="5xx"/>
        <code name="UNKNOWN_ERROR" http="other"/>
      </error-codes>
    </task>

    <task id="3" name="Register Stream Routes" acs="AC1">
      <description>Register stream routes in Express app</description>
      <files>
        <file action="modify">agent-server/src/index.ts</file>
      </files>
      <subtasks>
        <subtask>3.1 Import streamRouter</subtask>
        <subtask>3.2 Add app.use('/api/stream', streamRouter)</subtask>
      </subtasks>
    </task>

    <task id="4" name="Create Streaming Types" acs="AC1,AC8">
      <description>Define TypeScript types for streaming</description>
      <files>
        <file action="create">src/types/streaming.ts</file>
      </files>
      <subtasks>
        <subtask>4.1 Define StreamEventType union</subtask>
        <subtask>4.2 Define StreamEvent interface</subtask>
        <subtask>4.3 Define StreamState interface</subtask>
      </subtasks>
    </task>

    <task id="5" name="Create Streaming Service" acs="AC1,AC3,AC8">
      <description>Frontend service for consuming SSE streams</description>
      <files>
        <file action="create">src/lib/services/streamingService.ts</file>
      </files>
      <subtasks>
        <subtask>5.1 Implement SSE parsing from fetch response</subtask>
        <subtask>5.2 Handle all event types with callbacks</subtask>
        <subtask>5.3 Add abort capability for cancellation</subtask>
      </subtasks>
    </task>

    <task id="6" name="Create Tauri IPC Stream Forwarding" acs="AC8">
      <description>Rust command to forward SSE events to WebView</description>
      <files>
        <file action="create">src-tauri/src/stream_forwarder.rs</file>
        <file action="modify">src-tauri/src/main.rs</file>
      </files>
      <subtasks>
        <subtask>6.1 Create Rust command for starting stream</subtask>
        <subtask>6.2 Implement SSE parsing in Rust</subtask>
        <subtask>6.3 Emit Tauri events for each stream event</subtask>
        <subtask>6.4 Register command in Tauri plugin system</subtask>
      </subtasks>
      <rust-dependencies>
        <dep name="reqwest" version="0.12" features="stream"/>
        <dep name="futures-util" version="0.3"/>
      </rust-dependencies>
    </task>

    <task id="7" name="Update Chat Store for Streaming" acs="AC1,AC2,AC4,AC6">
      <description>Extend Zustand store with streaming state and actions</description>
      <files>
        <file action="modify">src/stores/chatStore.ts</file>
      </files>
      <subtasks>
        <subtask>7.1 Add streaming state variables (isStreaming, firstTokenReceived, etc.)</subtask>
        <subtask>7.2 Implement sendMessageStreaming action</subtask>
        <subtask>7.3 Implement appendToStream with first token tracking</subtask>
        <subtask>7.4 Implement completeStream with database persistence</subtask>
        <subtask>7.5 Add stream cancellation capability</subtask>
      </subtasks>
      <state-additions>
        <state name="isStreaming" type="boolean" default="false"/>
        <state name="currentStreamId" type="string | null" default="null"/>
        <state name="firstTokenReceived" type="boolean" default="false"/>
        <state name="firstTokenLatencyMs" type="number | null" default="null"/>
        <state name="streamStartTime" type="number | null" default="null"/>
        <state name="pendingAssistantContent" type="string" default="''"/>
      </state-additions>
    </task>

    <task id="8" name="Create Typing Indicator Component" acs="AC2">
      <description>Animated typing indicator shown before first token</description>
      <files>
        <file action="create">src/components/chat/TypingIndicator.tsx</file>
      </files>
      <subtasks>
        <subtask>8.1 Create component with three bouncing dots</subtask>
        <subtask>8.2 Style with Orion Design System tokens</subtask>
        <subtask>8.3 Show only before first token, hide after</subtask>
      </subtasks>
      <styling>
        <dot-color>orion-primary (#D4AF37)</dot-color>
        <text-color>orion-fg/50</text-color>
        <text-tracking>tracking-editorial (0.25em)</text-tracking>
        <animation>animate-bounce with staggered delays (0ms, 150ms, 300ms)</animation>
        <text>"Orion is thinking..."</text>
      </styling>
    </task>

    <task id="9" name="Create Streaming Text Component" acs="AC4">
      <description>Text component with streaming cursor</description>
      <files>
        <file action="create">src/components/chat/StreamingText.tsx</file>
      </files>
      <subtasks>
        <subtask>9.1 Render text with whitespace preservation</subtask>
        <subtask>9.2 Add streaming cursor indicator (blinking)</subtask>
        <subtask>9.3 Optimize re-renders during rapid updates</subtask>
      </subtasks>
    </task>

    <task id="10" name="Implement Auto-Scroll" acs="AC5">
      <description>Auto-scroll hook for chat container</description>
      <files>
        <file action="create">src/hooks/useAutoScroll.ts</file>
      </files>
      <subtasks>
        <subtask>10.1 Implement auto-scroll on new content</subtask>
        <subtask>10.2 Detect user manual scroll to pause auto-scroll</subtask>
        <subtask>10.3 Reset scroll state when streaming completes</subtask>
      </subtasks>
    </task>

    <task id="11" name="Update Message List for Streaming" acs="AC4,AC5,AC7">
      <description>Integrate streaming into message list</description>
      <files>
        <file action="modify">src/components/chat/MessageList.tsx</file>
      </files>
      <subtasks>
        <subtask>11.1 Integrate auto-scroll hook</subtask>
        <subtask>11.2 Show typing indicator at correct time</subtask>
        <subtask>11.3 Add "scroll to bottom" button when user scrolled up</subtask>
        <subtask>11.4 Pass streaming state to message bubbles</subtask>
      </subtasks>
    </task>

    <task id="12" name="Update Message Bubble for Streaming" acs="AC4">
      <description>Support streaming state in message bubble</description>
      <files>
        <file action="modify">src/components/chat/MessageBubble.tsx</file>
      </files>
      <subtasks>
        <subtask>12.1 Accept isStreaming prop</subtask>
        <subtask>12.2 Use StreamingText component for streaming messages</subtask>
        <subtask>12.3 Apply Orion Design System chat styles</subtask>
      </subtasks>
    </task>

    <task id="13" name="Update Chat Input for Streaming" acs="AC1,AC6">
      <description>Switch to streaming and add cancel button</description>
      <files>
        <file action="modify">src/components/chat/ChatInput.tsx</file>
      </files>
      <subtasks>
        <subtask>13.1 Switch from sendMessage to sendMessageStreaming</subtask>
        <subtask>13.2 Add cancel button during streaming</subtask>
        <subtask>13.3 Disable input during streaming</subtask>
      </subtasks>
    </task>

    <task id="14" name="Performance Monitoring" acs="AC3,AC7">
      <description>Track streaming performance metrics</description>
      <files>
        <file action="create">src/lib/performance/streamMetrics.ts</file>
      </files>
      <subtasks>
        <subtask>14.1 Track first token latency</subtask>
        <subtask>14.2 Track total tokens and tokens/second</subtask>
        <subtask>14.3 Log performance metrics for debugging</subtask>
      </subtasks>
    </task>

    <task id="15" name="Unit Tests" acs="AC2,AC3,AC4">
      <description>Unit tests for streaming components</description>
      <files>
        <file action="create">tests/unit/components/TypingIndicator.test.tsx</file>
        <file action="create">tests/unit/components/StreamingText.test.tsx</file>
        <file action="create">tests/unit/hooks/useAutoScroll.test.ts</file>
      </files>
    </task>

    <task id="16" name="Integration Tests" acs="AC1,AC6">
      <description>Integration tests for SSE endpoint</description>
      <files>
        <file action="create">tests/integration/streaming/sse-endpoint.test.ts</file>
      </files>
    </task>

    <task id="17" name="E2E Tests" acs="AC1,AC3,AC4,AC7">
      <description>End-to-end tests for streaming flow</description>
      <files>
        <file action="create">tests/e2e/streaming.spec.ts</file>
      </files>
    </task>

  </tasks>

  <!-- ============================================================ -->
  <!-- SECTION 5: FILE STRUCTURE -->
  <!-- ============================================================ -->
  <file-structure>
    <![CDATA[
agent-server/
├── src/
│   ├── services/
│   │   └── claude-client.ts      # MODIFY: Add streaming method
│   └── routes/
│       └── stream.ts             # MODIFY: Replace placeholder with real SSE

src/
├── types/
│   └── streaming.ts              # CREATE: Stream event types
├── lib/
│   ├── services/
│   │   └── streamingService.ts   # CREATE: Frontend streaming service
│   └── performance/
│       └── streamMetrics.ts      # CREATE: Performance tracking
├── hooks/
│   └── useAutoScroll.ts          # CREATE: Auto-scroll hook
├── stores/
│   └── chatStore.ts              # MODIFY: Add streaming state/actions
└── components/
    └── chat/
        ├── TypingIndicator.tsx   # CREATE: Typing dots
        ├── StreamingText.tsx     # CREATE: Text with cursor
        ├── MessageList.tsx       # MODIFY: Add auto-scroll
        ├── MessageBubble.tsx     # MODIFY: Support streaming
        └── ChatInput.tsx         # MODIFY: Use streaming, add cancel

src-tauri/src/
├── main.rs                       # MODIFY: Register stream_forwarder
└── stream_forwarder.rs           # CREATE: Tauri IPC streaming

tests/
├── unit/
│   └── components/
│       ├── TypingIndicator.test.tsx    # CREATE
│       └── StreamingText.test.tsx      # CREATE
├── integration/
│   └── streaming/
│       └── sse-endpoint.test.ts        # CREATE
└── e2e/
    └── streaming.spec.ts               # CREATE
    ]]>
  </file-structure>

  <!-- ============================================================ -->
  <!-- SECTION 6: ATDD CHECKLIST SUMMARY -->
  <!-- ============================================================ -->
  <atdd-summary>
    <total-tests>38</total-tests>
    <unit-tests>12</unit-tests>
    <integration-tests>8</integration-tests>
    <e2e-tests>14</e2e-tests>
    <performance-tests>4</performance-tests>

    <priority-breakdown>
      <p0>Most tests (SSE connection, typing indicator, first token latency, message persistence)</p0>
      <p1>Visual polish tests (cursor, whitespace preservation)</p1>
    </priority-breakdown>

    <key-test-ids>
      <test id="1.8-E2E-001">SSE connection established and receives data</test>
      <test id="1.8-E2E-002">Typing indicator appears before first token</test>
      <test id="1.8-E2E-005">First token latency measurement</test>
      <test id="1.8-E2E-013">Message persists after completion</test>
      <test id="1.8-E2E-014">Long response (1000+ tokens) streams smoothly</test>
      <test id="1.8-PERF-001">First token arrives within 500ms (p95)</test>
    </key-test-ids>

    <atdd-file>thoughts/implementation-artifacts/stories/atdd-checklist-1-8-streaming-responses.md</atdd-file>
  </atdd-summary>

  <!-- ============================================================ -->
  <!-- SECTION 7: DESIGN SYSTEM TOKENS -->
  <!-- ============================================================ -->
  <design-system name="Orion Design System">
    <colors>
      <color name="orion-primary" value="#D4AF37" usage="Bounce dots, streaming cursor"/>
      <color name="orion-fg" value="#1A1A1A" usage="Text color"/>
      <color name="orion-bg" value="#F5F5DC" usage="Background"/>
    </colors>
    <typography>
      <token name="tracking-editorial" value="0.25em" usage="Uppercase label letter-spacing"/>
    </typography>
    <components>
      <component name="User Message Bubble" style="bg-orion-fg text-orion-bg (black bg, cream text)"/>
      <component name="Agent Message Bubble" style="border-l-2 border-orion-primary bg-transparent (gold border)"/>
      <component name="Cancel Button" style="border border-red-500 text-red-500 hover:bg-red-500/10"/>
      <component name="Send Button" style="btn-gold-slide px-6 py-3"/>
    </components>
  </design-system>

  <!-- ============================================================ -->
  <!-- SECTION 8: PERFORMANCE TARGETS -->
  <!-- ============================================================ -->
  <performance-targets>
    <target metric="First token latency" value="less than 500ms p95" nfr="NFR-P001"/>
    <target metric="Render frame rate" value="60fps"/>
    <target metric="Memory stability" value="No unbounded growth during long streams"/>
    <target metric="Token throughput" value="20-50 tokens/second typical"/>
  </performance-targets>

  <!-- ============================================================ -->
  <!-- SECTION 9: IMPLEMENTATION NOTES -->
  <!-- ============================================================ -->
  <implementation-notes>

    <note category="SSE vs WebSocket">
      SSE was chosen over WebSocket because:
      1. Simpler implementation (unidirectional)
      2. Perfect for streaming text (no bidirectional needed)
      3. Native browser support via fetch + ReadableStream
      4. No additional dependencies required
    </note>

    <note category="Tauri Event Forwarding">
      The data flow is: Agent Server returns SSE to Tauri Rust process,
      Rust parses SSE and emits Tauri events, Frontend listens via
      @tauri-apps/api/event listen() function.
    </note>

    <note category="First Token Tracking">
      Record streamStartTime when request sent. Record firstTokenTime on
      first text event. Latency = firstTokenTime - streamStartTime.
      This is critical for NFR-P001 compliance.
    </note>

    <note category="Auto-Scroll Behavior">
      Scroll to bottom on new content IF user is at bottom.
      If user scrolls up, pause auto-scroll.
      Show "scroll to bottom" button when paused.
      Reset on stream completion.
    </note>

    <note category="Memory Management">
      Accumulate content in single string, not array.
      Update message in-place (immer handles efficiently).
      Clear pending content after stream completes.
    </note>

    <note category="Dependencies">
      No new npm dependencies required - uses existing @anthropic-ai/sdk streaming.
      Rust dependencies (reqwest with stream, futures-util) were already added in Story 1.5.
    </note>

  </implementation-notes>

  <!-- ============================================================ -->
  <!-- SECTION 10: TESTING REQUIREMENTS -->
  <!-- ============================================================ -->
  <testing>
    <frameworks>
      <framework name="Vitest" purpose="Unit and integration tests"/>
      <framework name="React Testing Library" purpose="Component testing"/>
      <framework name="Playwright" purpose="E2E tests"/>
    </frameworks>

    <environment-requirements>
      <requirement>ANTHROPIC_API_KEY environment variable for integration tests</requirement>
      <requirement>Agent Server running on port 3001 for integration tests</requirement>
      <requirement>Tests may incur API costs</requirement>
    </environment-requirements>

    <quality-gates>
      <gate name="Unit Tests" pass-criteria="100% pass" blocker="yes"/>
      <gate name="Integration Tests" pass-criteria="100% pass" blocker="yes"/>
      <gate name="E2E Tests" pass-criteria="100% pass" blocker="yes"/>
      <gate name="First Token Latency" pass-criteria="less than 500ms p95" blocker="yes"/>
      <gate name="Memory Stability" pass-criteria="No unbounded growth" blocker="yes"/>
      <gate name="Visual Regression" pass-criteria="less than 1% diff" blocker="no"/>
    </quality-gates>
  </testing>

  <!-- ============================================================ -->
  <!-- SECTION 11: BUTLER SYSTEM PROMPT -->
  <!-- ============================================================ -->
  <butler-system-prompt>
    <![CDATA[
You are Orion, an AI-powered personal butler assistant. You help users manage their digital lives by assisting with email, calendar, tasks, and relationships.

Be helpful, concise, and proactive. Ask clarifying questions when needed. Remember context from the conversation.
    ]]>
  </butler-system-prompt>

</story-context>
